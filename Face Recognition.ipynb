{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Face Recognition System\n",
    "\n",
    "Nowadays there are many ways of authenticating yourself, like using password, retina scan, fingerprint etc. Face can also be used for this purpose. In this notebook we will make a face recognition system using Siamese network.\n",
    "This is different from face verification where the task is to know whether given two input images are same or not.\n",
    "Here the task is see whether the given input image is of any person who is registered with the system or not. There can be multiple users registered with the system.\n",
    "\n",
    "The advantage of **Siamese Network** is that it allows a way to do this sort of verification task with very little user data, as it is quite unreasonable to train using thousands of images for each user. Here we will be using **FaceNet Model**.\n",
    "\n",
    "FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. So by comparing two such vectors, we can then determine if two pictures are of the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utility import *\n",
    "from inception_blocks_v2 import *\n",
    "from webcam_utils import *\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The model makes an encoding vector consisting of 128 numbers for the input image. Two encodings are compared and if the two encodings are similar then we say that the two images are of the same person otherwise they are different. \n",
    "The model uses **Triplet loss function**. The aim is to minimize this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss function\n",
    "#  y_pred - list containing three objects:\n",
    "#         anchor(None, 128) -- encodings for the anchor images\n",
    "#         positive(None, 128) -- encodings for the positive images\n",
    "#         negative(None, 128) -- encodings for the negative images\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # triplet formula components\n",
    "    pos_dist = tf.reduce_sum( tf.square(tf.subtract(y_pred[0], y_pred[1])) )\n",
    "    neg_dist = tf.reduce_sum( tf.square(tf.subtract(y_pred[0], y_pred[2])) )\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    \n",
    "    loss = tf.maximum(basic_loss, 0.0)\n",
    "   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "The model outputs a vector of 128 numbers which represent encoding for the given input image. We will be using this encoding vector for comparing two images.\n",
    "#### Input\n",
    "- This network takes as input 96x96 RGB image as its input. Specifically, inputs a tensor of shape $(m, n_C, n_H, n_W)$ , where $n_C$ = channel.\n",
    "\n",
    "#### Output\n",
    "- A matrix of shape **(m, 128)** where the 128 numbers are the encoding values for $ith$ image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the trained weights\n",
    "We will be using a pretrained model since it requires a lot of time and data for training such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved weights\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Database\n",
    "\n",
    "We will create a databse of registered. For this we will use a simple dictionary and map each registered user with his/her face encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add user Here using image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a dict for keeping track of ampping of each person with his/her face encoding\n",
    "user_db = {}\n",
    "# add a user\n",
    "user_db[\"susanta\"] = img_to_encoding(\"images/2.jpg\", FRmodel)\n",
    "user_db[\"person 2\"] = img_to_encoding(\"images/4.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "For making this face recognition system we are going to take the input image, find its encoding and then see if there is any similar encoding in the database or not. We define a threshold value to decide whether the two images are similar or not based on the similarity of their encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognize the user face by checking for it in the database\n",
    "def recognize_face(image_path, database, model):\n",
    "    # find the face encodings for the input image\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    min_dist = 99999\n",
    "    threshold = 0.7\n",
    "    # loop over all the recorded encodings in database \n",
    "    for name in database:\n",
    "        # find the similarity between the input encodings and claimed person's encodings using L2 norm\n",
    "        dist = np.linalg.norm(np.subtract(database[name], encoding) )\n",
    "        # check if minimum distance or not\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    if min_dist > threshold:\n",
    "        print(\"User not in the database.\")\n",
    "        identity = 'Unknown Person'\n",
    "    else:\n",
    "        print (\"Hi! \" + str(identity) + \", L2 distance: \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(database, model):\n",
    "    save_loc = r'saved_image/1.jpg'\n",
    "    capture_obj = cv2.VideoCapture(0)\n",
    "    capture_obj.set(3, 640)  # WIDTH\n",
    "    capture_obj.set(4, 480)  # HEIGHT\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        r'haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # whether there was any face found or not\n",
    "    face_found = False\n",
    "\n",
    "    # run the webcam for given seconds\n",
    "    req_sec = 3\n",
    "    loop_start = time.time()\n",
    "    elapsed = 0\n",
    "\n",
    "    while(True):\n",
    "        curr_time = time.time()\n",
    "        elapsed = curr_time - loop_start\n",
    "        if elapsed >= req_sec:\n",
    "            break\n",
    "        \n",
    "        # capture_object frame-by-frame\n",
    "        ret, frame = capture_obj.read()\n",
    "        # mirror the frame\n",
    "        frame = cv2.flip(frame, 1, 0)\n",
    "\n",
    "        # Our operations on the frame come here\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # detect face\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        for (x, y, w, h) in faces:\n",
    "            # required region for the face\n",
    "            roi_color = frame[y-90:y+h+70, x-50:x+w+50]\n",
    "            # save the detected face\n",
    "            cv2.imwrite(save_loc, roi_color)\n",
    "            # draw a rectangle bounding the face\n",
    "            cv2.rectangle(frame, (x-10, y-70), (x+w+20, y+h+40), (255, 0, 0), 2)\n",
    "\n",
    "        # display the frame with bounding rectangle\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # close the webcam when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # release the capture_object\n",
    "    capture_obj.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        face_found = True\n",
    "        \n",
    "    return face_found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# detects faces in realtime from webcam feed\n",
    "def detect_face_realtime(database, model, threshold=0.7):\n",
    "    text = ''\n",
    "    \n",
    "    save_loc = r'saved_image/1.jpg'\n",
    "    capture_obj = cv2.VideoCapture(0)\n",
    "    capture_obj.set(3, 640)  # WIDTH\n",
    "    capture_obj.set(4, 480)  # HEIGHT\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        r'haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    prev_time = time.time()\n",
    "    while(True):\n",
    "        \n",
    "        # capture_object frame-by-frame\n",
    "        ret, frame = capture_obj.read()\n",
    "        # mirror the frame\n",
    "        frame = cv2.flip(frame, 1, 0)\n",
    "\n",
    "        # Our operations on the frame come here\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # detect face\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        for (x, y, w, h) in faces:\n",
    "            # required region for the face\n",
    "            roi_color = frame[y-90:y+h+70, x-50:x+w+50]\n",
    "\n",
    "            # save the detected face\n",
    "            cv2.imwrite(save_loc, roi_color)\n",
    "            \n",
    "             # keeps track of waiting time for face recognition\n",
    "            curr_time = time.time()\n",
    "            \n",
    "            if curr_time - prev_time >= 5 and len(faces) > 0: \n",
    "                resize_img(\"saved_image/1.jpg\")\n",
    "                min_dist, identity, registered = recognize_face_realtime(\n",
    "                                                    save_loc, database, model, threshold)\n",
    "                print('working')\n",
    "                if min_dist <= threshold:\n",
    "                    # for putting text overlay on webcam feed\n",
    "                    text = 'Welcome ' + identity \n",
    "                else:\n",
    "                    text = ''\n",
    "                # save the time when the last face recognition task was done\n",
    "                prev_time = time.time()\n",
    "                \n",
    "            # draw a rectangle bounding the face\n",
    "            cv2.rectangle(frame, (x-10, y-70),\n",
    "                          (x+w+20, y+h+40), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, text, (50, 50), font, 2, (255, 255, 0), 2)\n",
    "        \n",
    "        # display the frame with bounding rectangle\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # close the webcam when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # release the capture_object\n",
    "    capture_obj.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# checks whether the input face is a registered user or not\n",
    "def recognize_face_realtime(image_path, database, model, threshold):\n",
    "    # find the face encodings for the input image\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    registered = False\n",
    "    min_dist = 99999\n",
    "\n",
    "    # loop over all the recorded encodings in database\n",
    "    for name in database:\n",
    "        # find the similarity between the input encodings and claimed person's encodings using L2 norm\n",
    "        dist = np.linalg.norm(np.subtract(database[name], encoding))\n",
    "        # check if minimum distance or not\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    if min_dist > threshold :\n",
    "        registered = False\n",
    "    else:\n",
    "        registered = True\n",
    "    return min_dist, identity, registered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "working\n",
      "working\n"
     ]
    }
   ],
   "source": [
    "detect_face_realtime(user_db, FRmodel, threshold = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the capture_object\n",
    "#capture_obj.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Run the face recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was no face in the visible frame. Try again...........\n"
     ]
    }
   ],
   "source": [
    "# we can use the webcam to capture the user image then get it recognized\n",
    "face_found = detect_face(user_db, FRmodel)\n",
    "if face_found:\n",
    "    resize_img(\"saved_image/1.jpg\")\n",
    "    recognize_face(\"saved_image/1.jpg\", user_db, FRmodel)\n",
    "else:\n",
    "    print('There was no face found in the visible frame. Try again...........')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- A lot of code is based on the assignment from Convolutional Neural Networks Specialization by Deeplearning.ai on Coursera.<br>\n",
    "https://www.coursera.org/learn/convolutional-neural-networks/home/welcome \n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- The pretrained model used is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- A lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
